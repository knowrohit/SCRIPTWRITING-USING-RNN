{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (2.8.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.22.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.44.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.24.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (13.0.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/rohittiwari/opt/anaconda3/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whilst seeing the output make sure you are aware that the machine laerning model didnt know what an english word was!!\n",
    "\n",
    "\n",
    "* The structure of the output = a blocksof text  begin with a speaker name which i inputted in the model, in all capital letters similar to the dataset.\n",
    "\n",
    "* As seeen below, the model is trained on small and large batches of text (100 characters each) and is still able to generate a longer sequence of text with not- so ye in it hahahaha!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('SCI-FI.txt', 'http://www.scifiscripts.com/scripts/5thelement.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 162771 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tThe Fifth Element\n",
      "\n",
      "\t\t\t\tAn original script\n",
      "\t\t\t\tby\n",
      "\t\t\t\tLuc Besson\n",
      "\n",
      "\t\t\t\tRevisions by\n",
      "\t\t\t\tLuc Besson\n",
      "\t\t\t\tand\n",
      "\t\t\t\tRobert Mark Kamen\n",
      "\n",
      "\t\t\t\tAugust 1995 Draft\n",
      "\n",
      "\t\t\t\tGaumont and Les Films du Dauphin\n",
      "\n",
      "FADE IN:\n",
      "\n",
      "1\tEXT.  DESERT  NILE RIVER  VA\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the texts,\n",
    "\n",
    "Before training, you need to convert the strings to a numerical representation. \n",
    "\n",
    "The ''tf.keras.layers.StringLookup'' layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "It converts from tokens to character IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[53, 54, 55, 56, 57, 58, 59], [76, 77, 78]]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inverting this represeentation to human-readable strings is the main task at hand. For this you can use `tf.keras.layers.\n",
    "\n",
    "1.   List item\n",
    "2.   List item\n",
    "\n",
    "StringLookup(..., invert=True)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTING NEXT PROBABLE WORD\n",
    "\n",
    "Since RNNs maintain an internal state of neurons like cnns that depends on the previously seen elements, given all the characters computed until this moment, we might or might not use \"ensembling leaning\" next time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
    "\n",
    "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
    "\n",
    "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
    "\n",
    "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids\n",
    "\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "F\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `batch` method lets you easily convert these individual characters to sequences of the desired size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'\\t' b'\\t' b'\\t' b'\\t' b'T' b'h' b'e' b' ' b'F' b'i' b'f' b't' b'h' b' '\n",
      " b'E' b'l' b'e' b'm' b'e' b'n' b't' b'\\r' b'\\n' b'\\r' b'\\n' b'\\t' b'\\t'\n",
      " b'\\t' b'\\t' b'A' b'n' b' ' b'o' b'r' b'i' b'g' b'i' b'n' b'a' b'l' b' '\n",
      " b's' b'c' b'r' b'i' b'p' b't' b'\\r' b'\\n' b'\\t' b'\\t' b'\\t' b'\\t' b'b'\n",
      " b'y' b'\\r' b'\\n' b'\\t' b'\\t' b'\\t' b'\\t' b'L' b'u' b'c' b' ' b'B' b'e'\n",
      " b's' b's' b'o' b'n' b'\\r' b'\\n' b'\\r' b'\\n' b'\\t' b'\\t' b'\\t' b'\\t' b'R'\n",
      " b'e' b'v' b'i' b's' b'i' b'o' b'n' b's' b' ' b'b' b'y' b'\\r' b'\\n' b'\\t'\n",
      " b'\\t' b'\\t' b'\\t' b'L' b'u' b'c' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\t\\t\\t\\tThe Fifth Element\\r\\n\\r\\n\\t\\t\\t\\tAn original script\\r\\n\\t\\t\\t\\tby\\r\\n\\t\\t\\t\\tLuc Besson\\r\\n\\r\\n\\t\\t\\t\\tRevisions by\\r\\n\\t\\t\\t\\tLuc '\n",
      "b'Besson\\r\\n\\t\\t\\t\\tand\\r\\n\\t\\t\\t\\tRobert Mark Kamen\\r\\n\\r\\n\\t\\t\\t\\tAugust 1995 Draft\\r\\n\\r\\n\\t\\t\\t\\tGaumont and Les Films du Dauph'\n",
      "b'in\\r\\n\\r\\nFADE IN:\\r\\n\\r\\n1\\tEXT.  DESERT  NILE RIVER  VALLEY - DAY\\r\\n\\r\\n\\tSomewhere in the Nile at the edge of t'\n",
      "b'he desert.\\r\\n\\r\\n\\tCREDITS  ROLL\\r\\n\\r\\n\\tWRITTEN:\\tEGYPT 1913\\r\\n\\r\\n\\tOMAR and his mule zigzag along the bottom of'\n",
      "b' sun scorched dunes.\\r\\n\\r\\n2\\tEXT.  TEMPLE  EXCAVATION - DAY\\r\\n\\r\\n\\tThe mule and the boy finally reach a cam'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training you'll need a dataset of `(input, label)` pairs. Where `input` and \n",
    "`label` are sequences. At each time step the input is the current character and the label is the next character. \n",
    "\n",
    "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['h', 'u', 'n', 't', 'e', 'r', 'x', 'h', 'u', 'n', 't', 'e', 'r'],\n",
       " ['u', 'n', 't', 'e', 'r', 'x', 'h', 'u', 'n', 't', 'e', 'r', 's'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"hunterxhunters\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'\\t\\t\\t\\tThe Fifth Element\\r\\n\\r\\n\\t\\t\\t\\tAn original script\\r\\n\\t\\t\\t\\tby\\r\\n\\t\\t\\t\\tLuc Besson\\r\\n\\r\\n\\t\\t\\t\\tRevisions by\\r\\n\\t\\t\\t\\tLuc'\n",
      "Target: b'\\t\\t\\tThe Fifth Element\\r\\n\\r\\n\\t\\t\\t\\tAn original script\\r\\n\\t\\t\\t\\tby\\r\\n\\t\\t\\t\\tLuc Besson\\r\\n\\r\\n\\t\\t\\t\\tRevisions by\\r\\n\\t\\t\\t\\tLuc '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training batches\n",
    "\n",
    "You used `tf.data` to split the text into manageable sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###BUILD THE MODEL\n",
    "\n",
    "This section defines the model as a `keras.Model` subclass (For details see \n",
    "This model has three layers:\n",
    "\n",
    "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
    "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use an LSTM layer here.)\n",
    "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model.\n",
    "\n",
    "\n",
    "^^^ go through each type of rnn to learn the dense layers of embedding. MOST IMP if feeding data into bernard ansd spitting out relevant to the point output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: For training you could use a `keras.Sequential` model here. To  generate text later you'll need to manage the RNN's internal state. It's simpler to include the state input and output options upfront, than it is to rearrange the model architecture later. For more details see the [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the model\n",
    "\n",
    "Now run the model to see that it behaves as expected.\n",
    "\n",
    "First check the shape of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 79) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  20224     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  80975     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,039,503\n",
      "Trainable params: 4,039,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
    "\n",
    "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
    "\n",
    "Try it for the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us, at each timestep, a prediction of the next character index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34, 76, 32, 47, 34, 66, 27, 45,  7, 24, 23, 62, 46,  6, 13, 34, 10,\n",
       "       69, 38, 18, 24, 46, 14, 17, 20, 31, 59,  2, 49, 59, 35, 74, 39,  2,\n",
       "       48,  1,  8, 18, 16, 49, 68, 62, 55, 58, 72, 29, 21, 63, 48, 26, 24,\n",
       "        9, 54, 54, 14, 36,  4, 61,  7, 19, 39, 42, 12, 67, 68, 39,  1, 56,\n",
       "       24, 73, 36, 14, 78,  8, 60, 48, 40, 59, 32,  5, 24, 70, 26, 77, 23,\n",
       "        6, 15, 44, 31, 17, 60, 28, 15, 30, 46, 39, 62,  8, 67, 47])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode these to see the text predicted by this\n",
    "\n",
    "untrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b\"ho in reloading.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tEMPEROR\\r\\n\\t\\t\\t\\tI'd say about thirty yards to the left.\\r\\n\\r\\n\\tKorben hefts the \"\n",
      "\n",
      "Next Char Predictions:\n",
      " b'HxFUHnAS\\':9jT\"/H,qL4:T036Eg\\nWgIvM\\nV\\t(42WpjcftC7kV?:)bb0J i\\'5MP.opM\\td:uJ0z(hVNgF!:r?y9\"1RE3hB1DTMj(oU'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN THE MODEL\n",
    "\n",
    "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATTACH THE OPTIMIZER AND LOSS FUNCTION\n",
    "\n",
    "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
    "\n",
    "Because your model returns logits, you need to set the `from_logits` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 79)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.3697495, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.023834"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the training procedure using the `tf.keras.Model.compile` method. Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIGURE CHECKPOINTS\n",
    " Use a tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe7a63b5070>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = tf.train.Checkpoint(model)\n",
    "\n",
    "# Save a checkpoint to /tmp/training_checkpoints-{save_counter}. Every time\n",
    "# checkpoint.save is called, the save counter is increased.\n",
    "save_path = checkpoint.save('/tmp/training_checkpoints')\n",
    "\n",
    "# Restore the checkpointed values to the `model` object.\n",
    "checkpoint.restore(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = '/tmp/training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXECUTE THE TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "25/25 [==============================] - 30s 1s/step - loss: 4.1154\n",
      "Epoch 2/35\n",
      "25/25 [==============================] - 33s 1s/step - loss: 2.9296\n",
      "Epoch 3/35\n",
      "25/25 [==============================] - 34s 1s/step - loss: 2.5572\n",
      "Epoch 4/35\n",
      "25/25 [==============================] - 35s 1s/step - loss: 2.3322\n",
      "Epoch 5/35\n",
      "25/25 [==============================] - 31s 1s/step - loss: 2.2006\n",
      "Epoch 6/35\n",
      "25/25 [==============================] - 36s 1s/step - loss: 2.0964\n",
      "Epoch 7/35\n",
      "25/25 [==============================] - 38s 2s/step - loss: 2.0043\n",
      "Epoch 8/35\n",
      "25/25 [==============================] - 35s 1s/step - loss: 1.9201\n",
      "Epoch 9/35\n",
      "25/25 [==============================] - 34s 1s/step - loss: 1.8439\n",
      "Epoch 10/35\n",
      "25/25 [==============================] - 34s 1s/step - loss: 1.7738\n",
      "Epoch 11/35\n",
      "25/25 [==============================] - 35s 1s/step - loss: 1.7130\n",
      "Epoch 12/35\n",
      "25/25 [==============================] - 36s 1s/step - loss: 1.6507\n",
      "Epoch 13/35\n",
      "25/25 [==============================] - 34s 1s/step - loss: 1.5918\n",
      "Epoch 14/35\n",
      "25/25 [==============================] - 32s 1s/step - loss: 1.5378\n",
      "Epoch 15/35\n",
      "25/25 [==============================] - 34s 1s/step - loss: 1.4872\n",
      "Epoch 16/35\n",
      "25/25 [==============================] - 38s 2s/step - loss: 1.4384\n",
      "Epoch 17/35\n",
      "25/25 [==============================] - 36s 1s/step - loss: 1.3903\n",
      "Epoch 18/35\n",
      "25/25 [==============================] - 36s 1s/step - loss: 1.3410\n",
      "Epoch 19/35\n",
      "25/25 [==============================] - 40s 2s/step - loss: 1.2930\n",
      "Epoch 20/35\n",
      "25/25 [==============================] - 44s 2s/step - loss: 1.2457\n",
      "Epoch 21/35\n",
      "25/25 [==============================] - 46s 2s/step - loss: 1.2008\n",
      "Epoch 22/35\n",
      "25/25 [==============================] - 41s 2s/step - loss: 1.1550\n",
      "Epoch 23/35\n",
      "25/25 [==============================] - 36s 1s/step - loss: 1.1098\n",
      "Epoch 24/35\n",
      "25/25 [==============================] - 34s 1s/step - loss: 1.0637\n",
      "Epoch 25/35\n",
      "25/25 [==============================] - 35s 1s/step - loss: 1.0181\n",
      "Epoch 26/35\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.9735\n",
      "Epoch 27/35\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.9239\n",
      "Epoch 28/35\n",
      "25/25 [==============================] - 35s 1s/step - loss: 0.8765\n",
      "Epoch 29/35\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.8263\n",
      "Epoch 30/35\n",
      "25/25 [==============================] - 33s 1s/step - loss: 0.7750\n",
      "Epoch 31/35\n",
      "25/25 [==============================] - 33s 1s/step - loss: 0.7239\n",
      "Epoch 32/35\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.6746\n",
      "Epoch 33/35\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.6225\n",
      "Epoch 34/35\n",
      "25/25 [==============================] - 34s 1s/step - loss: 0.5705\n",
      "Epoch 35/35\n",
      "25/25 [==============================] - 33s 1s/step - loss: 0.5189\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.\n",
    "\n",
    "![To generate text the model's output is fed back to the input](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_sampling.png?raw=1)\n",
    "\n",
    "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it in a loop to generate some \n",
    "\n",
    "---\n",
    "\n",
    "text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a OG writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROHIT:\n",
      "\t\t\t\t..Cimating arrous!  The Pleese for two\n",
      "\t\t\t\tand Destroying but.\n",
      "\n",
      "\t\t\t\t\t\tPRESIDENT\n",
      "\t\t\t\tRIGlook aftaid ither the hall.\n",
      "\n",
      "\t\t\t\t\t\tMOTHER (V.O.)\n",
      "\t\t\t\tHill the proble, you doing the lase, Mr. President, peeden\n",
      "\t\t\t\tMamious is non a moss man. Sell me minute! Megat time!\n",
      "\n",
      "\tThe ZFX ant hands to a plame light for Korben attEV\n",
      "\n",
      "212BINT.  KORBEN'S  APARTMENT - DAY\n",
      "\n",
      "\tThe Professor is lowned up.  There's onver control.  Leeloo points lead of crombaro an, oberingats like me.\n",
      "\n",
      "\t\t\t\t\t\tKORBEN\n",
      "\t\t\t\t\t(uppassing here, paws how to flyseed, Ge're all eath\n",
      "\t\t\t\tand color. One all weashed that. Con AND wanting\n",
      "\t\t\t\timportance. The President Juth for the Taltion\n",
      "\t\t\t\tint Korben Dallas Will acrops the Stones...\n",
      "\t\t\t\tSoun our day gottang never tike?\n",
      "\n",
      "\t\t\t\t\t(turn light)\n",
      "\t\t\t\tI'm laterally. General?\n",
      "\n",
      "\t\t\t\t\tGEMANDER\n",
      "\t\t\t\tFornelius, presistures if I didn't fire of\n",
      "\t\t\t\tthis isn't the cam go one it.\n",
      "\n",
      "\tThe COP Solice hall call one in my face, but it's moving.\n",
      "\n",
      "2.1\tINT.  COCKEIT  FHLOSTON  PARADISE  LOUNDE  LABORATORYE\n",
      "\n",
      "\tThe piloth goes off.  DIVA'S  She opens a card in front of her albs...\n",
      "\n",
      "51\tINT.  MONDOSHAWAN  SOLINE - DAY\n",
      "\n",
      "\tA gug stop its preased with a stap of it, specials ont\n",
      "off, voice in tool see war doo's \tImsiles.  A vast\n",
      "more piles of laughber and we ske them.\n",
      "\n",
      "31\tINT.  CONCERT  HALL\n",
      "\n",
      "\tLeeloo explaises hes in a box.  A BOT - OMIT\n",
      "\n",
      "67\tINT.  FHLOSTON  PARADISE  SPACE - NIGHT\n",
      "\n",
      "\tCUGTER Cornelius starts to love as it anybvatar\n",
      "and belied on: Zorg walls up the crusication sene.\n",
      "\tShe is down the sight of champagne, screaming them another concentrate.\n",
      "\n",
      "\t\t\t\t\t\tCORNELIUS\n",
      "\t\t\t\tThanks, kide your exceptray consondant\n",
      "\t\t\t\tand special templem.\n",
      "\n",
      "\tStart me me, the signable hall to proy aralt me..\n",
      "\n",
      "\tAlloo legs the SOUnD opens a door to floor their comes up.\n",
      "\n",
      "\t\t\t\t\t\tKORBEN\n",
      "\t\t\t\tI have a door oat of life! All we're\n",
      "\t\t\t\thanding andorm and to lava apound their mission.\n",
      "\t\t\t\tThanks.  You, just as the Diva's that seen back.\n",
      "\n",
      "\tLeeloo rebeals her to his feet, scans the Supreme Counn walk.  Leeloo smiles, wakes Lie\n",
      "\tCornelius and Leeloo and sten\n",
      "mectangurt, theory at delenal around the corner.\n",
      "\n",
      "\t\t\t\t\t\tCORNELIUS\n",
      "\t\t\t\t...What do you mean?\n",
      "\n",
      "\tThO CLOSE his still can come and stugners at Dorro.  She looks into\n",
      "at the bomb.\n",
      "\n",
      "\t\t\t\t\t\tLOC RHOD\n",
      "\t\t\t\tAKNOT, FLLessone is Tho flame burother.\n",
      "\n",
      "62\tINT.  AIRPORT\n",
      "\n",
      "\tThe KOmmand taking to the cealing a computer\n",
      "carmy.  in drapsed's fall, whill staption actlesibobs.\n",
      "\n",
      "119\tINT.  CONCERT  HALL\n",
      "\n",
      "\tThe BOGVOS Appeadert un all whote deep tropplead leretably into the room\n",
      "on a panic.  Ammsone faser\n",
      "as if freetible, which slips apong the cross and presses a suble.\n",
      "\tMinable read\n",
      "the way on exerciees and shatters to connact the Stone.  Loc Rhod and Fallas everything\n",
      "him a res greature, blocks ot a hand cropp starys forming before shouting.\n",
      "\n",
      "286\tINT.  PRESIDENT'S  OFFICE\n",
      "\n",
      "\tPresident Lindberg opens even face, her weak, Korben all doget on him.\n",
      "\tHis atmase point Lacrie, PICESSO.\n",
      "\t\t\t\t\t(to DOSSOF)\tCornelius will be just stopp.\n",
      "\tOn the temple is collad\n",
      "into the altar in his face, his eye\n",
      "but the bomy listed up the pars.\n",
      "\n",
      "227\tINT.  CHAPEL\n",
      "\n",
      "\t\t\t\t\t\tZORG\n",
      "\t\t\t\t...I'll call.\n",
      "\n",
      "\t\t\t\t\t\tSHADOW\n",
      "\t\t\t\tLeeloo?.. in the never with you.\n",
      "\t\t\t\tnot the probe and consate there are then's\n",
      "\t\t\t\tabout a dive timing that!\n",
      "\n",
      "2121\tINT.  CHAPEL\n",
      "\n",
      "\tThe Diva's alm in the check-in conexatalt chicken and presses a smell anders with a smile.\n",
      "\n",
      "\t\t\t\t\t\tGROUND COP\n",
      "\t\t\t\t\t(weak)\n",
      "\t\t\t\tWo sue. Thank you to Flovens it is...\n",
      "\t\t\t\tnever sknut!\n",
      "\n",
      "\t\t\t\t\t\tZORG\n",
      "\t\t\t\tHis dreams!\n",
      "\n",
      "\tA place flight fore rings on the smip.\n",
      "\n",
      "\t\t\t\t\t\tSCAETAIN\n",
      "\t\t\t\tTell atay...\n",
      "\n",
      "\tThe President speps a light fat.  Cornelius wetches Leeloo in dongains.\n",
      "\n",
      "\t\t\t\t\t\tKORBEN\n",
      "\t\t\t\tJo! It wear of perfect!\n",
      "\t\t\t\tThanks for the laustheresion.. exally cared\n",
      "\t\t\tast um a way to Bid. Lecoma!\n",
      "\n",
      "\tCorn lights up the planet,  Imazing cabbly at the earong 900 blooding to\n",
      "\t\t\t\tman. The falls for the concert!\n",
      "\n",
      "\tThe President do! I hear be oner some crosses the door and wraps her in phone away.\n",
      "\n",
      "\t\t\t\t\t\tKORBEN\n",
      "\t\t\t\tYou hear there that two.\n",
      "\n",
      "\tCornelius and Loc Rhod are wead's Korben shrull around his peacond.\n",
      "\n",
      "5\tINT.  ROOM - DAY - OMIT\n",
      "\n",
      "47\tINT.  GARREHO - OMIT\n",
      "\n",
      "87\tINT.  PRESIDENT'S  OFFICE - DAY\n",
      "\n",
      "\tAknot, Zorg 1, and gives Leeloo's arm and smashes his way formeribally looks on\n",
      "temple rushing than stroks at the Stone.  ImeanR He pouls did a\n",
      "small Please just as the President, the temple empicater thes come wercong\n",
      "toward the ship.\n",
      "\n",
      "\t\t\t\t\t\tPRESIDENT\n",
      "\t\t\t\t\t(pleashen, Loc Rhod)\n",
      "\t\t\t\tGon't... I bet your still around the kinly take\n",
      "\t\t\t\tachit that's allear exciteden.\n",
      "\t\t\t\tFather, when, Sudeay!\n",
      "\n",
      "\tLeeloo innocendly starts to in the Professor.  Two case end remains to Biving the\n",
      "wall with a human stops into opening.\n",
      "\n",
      "\t\t\t\t\t\tZORG\n",
      "\t\t\t\tHere let me here to leary macter..\"\n",
      "\t\t\t\tI have I foor ow me?\n",
      "\t\t\t\tI don't une. The Zorg 1000 issill as\n",
      "every with the shuttle's across the can't sever explasing.  He looks at his sets the Sucref The cops are  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 10.475754976272583\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROHIT:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(5000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest thing you can do to improve the results is to train it for longer (try `EPOCHS = 30`).\n",
    "\n",
    "You can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions.\n",
    "\n",
    "If you want the model to generate text *faster* the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROHIT: FHLOSTON  PARADISE - DAY\\r\\n\\r\\n\\tLeeloo is armuns.  A few alien the farryore platesull.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tLOC RHOD\\r\\n\\t\\t\\t\\tKorben sonight aseed)\\r\\n\\t\\t\\t\\tYou'll leadn, it war! If IGe's the missile.\\r\\n\\r\\n\\tGave and his excroles from the flame tower day.\\r\\n\\r\\n6\\tINT.  TEMPLE  ROOM - DABAN\\r\\n\\r\\n\\tA going space in close?  Leeloo is awarm. Bolich stands to pun in the chamicated.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tKORBEN\\r\\n\\t\\t\\t\\tAny eveny, De forget it?\\r\\n\\r\\n\\tLoc Rhod hopsing the TaxC TIL  LONDANG  LONDELORGH\\r\\n\\r\\n\\tThe right is loaking at 50, one book.  Leeloo looks left on.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tKORBEN\\r\\n\\t\\t\\t\\tLife 1-, I'll be abrow.\\r\\n\\r\\n\\tHe emperse string them one questiona delicate a case.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tKORBEN\\r\\n\\t\\t\\t\\tTalk a million -of I- make\\r\\n\\t\\t\\tconst left on your latcher.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tKORBEN\\r\\n\\t\\t\\t\\tSo no open for lack. Fither a wellone)\\r\\n\\t\\t\\t\\t...Wh cave  I was or a class!\\r\\n\\t\\t\\t\\tit's how daybuh?\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tPRESIDENT\\r\\n\\t\\t\\t\\tBettand, excenling on pellea of it.\\r\\n\\r\\n\\tHe hands Cornelius to sho the Leolbol on her\\r\\nbeam, and a towel of a moment, the restable flamilat, dreashes off the hear\"\n",
      " b\"RAHUL:N wroppisheden.\\r\\n\\t\\t\\t\\tAnd even hear, vire figutes 5, we're all take\\r\\n\\t\\t\\t\\thearing fire a small to do if of heres.\\r\\n\\t\\t\\t\\tOne more deer orenal.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tLEELOO\\r\\n\\t\\t\\t\\tForget the Mankalous. Connelius whose and time\\r\\n\\t\\t\\t\\tor again stand it weter.  Lelloo.\\r\\n\\r\\n211\\tINT.  PRESIDENT'S  OFFICE\\r\\n\\r\\n\\tPoreay and\\r\\nrevealy Cheefoor.  Furrol out all of the bar.  Omazing to Korben is perible.\\r\\n\\tMactilburgh looks troubled.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tFOG\\r\\n\\t\\t\\t\\tWhat are you?\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tCORNELIUS\\r\\n\\t\\t\\t\\tWry taking the tem in the name? like an\\r\\n\\t\\t\\t\\tover by treation.. He from here.\\r\\n\\r\\n\\tThe FESSESTAWM NELLits an enecknor, who Menaweres, head cwiling all around Loc\\r\\nRhed on the SOUNE Zorg's jent at De somewhates to legg\\r\\nto the tirn, sleak at her with a 500 2Z, 29 21S POLICEAN  AIDERT a peace control paticulaty.  The mirror resident, Leeloo inconstruite.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tKORBEN\\r\\n\\t\\t\\t\\t\\t(ast in on maragh)\\r\\n\\t\\t\\t\\t...In't fever seen staring the far.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tKORBEN\\r\\n\\t\\t\\t\\tFoun our calm)Bit!\\r\\n\\r\\n\\t\\t\\t\\t\\tZORG\\r\\n\\t\\t\\t\\tAll what cup the sucradical lase?\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tEMPER\"\n",
      " b\"BERNARD:\\r\\n\\t\\t\\t\\t...Riyel mo, I'll talking you to the\\r\\n\\t\\t\\t\\tgive here thing\\r\\n\\t\\t\\t\\tand something to suar Cornelius..\\r\\n\\t\\t\\t\\tI got on way..I'm rogg de your dasy control.\\r\\n\\t\\t\\t\\tPermanot!\\r\\n\\r\\n49\\tINT.  COCKPIT - NIGHT - DAY\\r\\n\\r\\n\\tKorben's grabs the crapel.\\r\\nSo CEELER (O.S.)\\r\\n\\t\\t\\tE. police with menara in There's- He\\r\\n\\t\\t\\t\\tRhon'm so nucharge, but... the Stones...\\r\\n\\t\\t\\t\\tHe wons to pick out a gretal of form\\r\\n\\t\\t\\t\\ta targ)\\r\\n\\t\\t\\t\\t...The Stone openifal LA1IG, All fastible.\\r\\n\\t\\t\\t\\tAnd the snaps speed from 5 to 7! Shoosand and the 4kmort\\r\\n\\t\\t\\t\\tmy life! Mactilburgh!\\r\\n\\r\\n\\tHe cound to at the David's sheers. He hands fire.  Cornelius\\r\\nwraps away relens.  Answert to it like\\r\\na button race.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tCORNELIUS\\r\\n\\t\\t\\t\\t\\t(to the conne)\\r\\n\\t\\t\\t\\tI got no tureht, being attastention. He robots that two\\r\\n\\t\\t\\t...Sole perrised Fifu.  Right Ar0\\r\\n\\r\\n\\t\\t\\t\\t\\tZORG\\r\\n\\t\\t\\t\\tI be going to see if tryyted!\\r\\n\\r\\n\\tThe SPAROM ROD  COLNELIUS'S  SPACESHOF\\r\\n\\r\\n\\tLeeloo is peltcabon to hall through hold the cockroach made, Mr. Zorg.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tVOICE\\r\\n\\t\\t\\t\\tDivay Walr?\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tZOR\"\n",
      " b\"SOLOMON: COPKERS LAERAORB MUNGH, pauses to panic.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tZORG\\r\\n\\t\\t\\t\\t...Imp... we're working...\\r\\n\\t\\t\\t\\twe are lose! It must be weel!\\r\\n\\r\\n\\tThe Dark Planet closes the walla in his head.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tKORBEN\\r\\n\\t\\t\\t\\tCome on! You're nowing you..\\r\\n\\t\\t\\t\\tYou negged for your coopersman...\\r\\n\\r\\n\\tThe President cleares the musscrould.  Leeloo smiles\\r\\naway relenally left. She comes in a chain.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tFOG\\r\\n\\t\\t\\t\\tWhat obe doing please...\\r\\n\\r\\n\\tLoc Rhod nevers rist one.  Korben is still bitch.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tPRESIDENT\\r\\n\\t\\t\\t\\tWalkcalt?\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tCORNELIUS\\r\\n\\t\\t\\t\\tThanks for a man overemans.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tZORG\\r\\n\\t\\t\\t\\t\\t(loudly)\\r\\n\\t\\t\\t\\tMaybe wonked you wlat?\\r\\n\\r\\n\\tHe tokes that Cornelius to scream behind the compuinating and\\r\\nbruathing, beam beautime.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tSTAEDERT\\r\\n\\t\\t\\t\\tBrink tw me restions!\\r\\n\\r\\n147\\tINT.  ZORG'S  OFFICE - OMIT\\r\\n\\r\\n84\\tINT.  WAREHE - DUSK - OMIT\\r\\n\\r\\n41\\tINT.  TANIRE  LEDLY Youn slap with 295.. Ev, your each\\r\\nby the floor.\\r\\n\\r\\n\\tKorben turns, the COPS on a\\r\\nmidrow blasted into screechip.  The copp oper in her eyes.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tPROFESSOR\\r\\n\\t\\t\\t\"\n",
      " b\"DOLORES:\\r\\n\\r\\n\\tDavid goes to pur no hers fall and Korben Paradise.\\r\\n\\r\\n219\\tINT.  LACERITM NGIT\\r\\n\\r\\n\\tLeeloo doesn't haldly and swokes the kiragh and finishes dvareras.\\r\\n\\t\\t\\t\\tHo ophieated you lan't cell me introduce.\\r\\n\\r\\n\\tKorben stands appears in the fict\\r\\ncrims.  Blinding happensting and ages its wenks over.  He looks at the screen.  He\\r\\ndinable start, the only -will at the honey, sinally cannot\\r\\nknow who oug it our.  The cops into a wive all over the bar.\\r\\n\\r\\n11\\tINT.  TUNNEL - DAY\\r\\n\\r\\n\\tThe heat in cove mescage in a plasion cart.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tKORBEN\\r\\n\\t\\t\\t\\t\\t(eken)\\r\\n\\t\\t\\t\\tlangus is to hip out do we sne knowing you a\\r\\n\\t\\t\\t\\tganding stant cousting away...\\r\\n\\r\\n234\\tEXT.  TEMESE - DANG\\r\\n\\r\\n\\tTeries openn.  Korben ages a small roomise plane, like a full planet, and takes a power afferring there!\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tGIRL\\r\\n\\t\\t\\t\\tO..wh...\\r\\n\\r\\n\\tLeeloo looks back torther.  Korben pushes a bullying larger.\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tKORBEN\\r\\n\\t\\t\\t\\tJutt. We have home?\\r\\n\\r\\n\\t\\t\\t\\t\\t\\tCORNELIUS\\r\\n\\t\\t\\t\\t\\t(to the cat)\\r\\n\\t\\t\\t\\tI'm to go a finger, just be step it!\\r\\n\\r\\n\\tCornelius ge\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.0199697017669678\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROHIT:', 'RAHUL:', 'BERNARD:', 'SOLOMON:', 'DOLORES:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the generator\n",
    "\n",
    "This single-step model can easily be [saved and restored](https://www.tensorflow.org/guide/saved_model), allowing you to use it anywhere a `tf.saved_model` is accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fe78b927ca0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bernard:\n",
      "\t\t\t\tComeround an ears out him...\n",
      "\n",
      "\tHe pots the CLieftoon it in his hands.  In the chall-drests, \n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['bernard:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced: Customized Training\n",
    "\n",
    "\n",
    "The above training procedure is simple, but does not give you much control. It uses teacher-forcing which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes.\n",
    "\n",
    "So now that you've seen how to run the model manually next you'll implement the training loop. This gives a starting point if, for example, you want to implement curriculum learning to help stabilize the model's open-loop output.\n",
    "\n",
    "The most important part of a custom training loop is the train step function.\n",
    "\n",
    "Use tf.GradientTape to track the gradients. You can learn more about this approach by reading the eager execution guide.\n",
    "\n",
    "The basic procedure is:\n",
    "\n",
    "Execute the model and calculate the loss under a tf.GradientTape.\n",
    "Calculate the updates and apply them to the model using the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTraining(MyModel):\n",
    "  @tf.function\n",
    "  def train_step(self, inputs):\n",
    "      inputs, labels = inputs\n",
    "      with tf.GradientTape() as tape:\n",
    "          predictions = self(inputs, training=True)\n",
    "          loss = self.loss(labels, predictions)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "      return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "25/25 [==============================] - 33s 1s/step - loss: 4.0507\n",
      "Epoch 2/5\n",
      "25/25 [==============================] - 31s 1s/step - loss: 2.9196\n",
      "Epoch 3/5\n",
      "25/25 [==============================] - 31s 1s/step - loss: 2.5367\n",
      "Epoch 4/5\n",
      "25/25 [==============================] - 30s 1s/step - loss: 2.3142\n",
      "Epoch 5/5\n",
      "25/25 [==============================] - 34s 1s/step - loss: 2.1870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe786b44a60>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n",
    "    \n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "model.fit(dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'bernard:'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'\\r'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'\\n'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'\\t'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'\\t'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'\\t'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'\\t'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'C'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'o'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'm'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'e'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'r'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'o'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'u'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'n'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'd'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b' '], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'a'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'n'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b' '], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'e'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'a'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'r'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b's'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b' '], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'o'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'u'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b't'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b' '], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'h'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'i'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'm'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'.'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'.'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'.'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'\\r'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'\\n'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'\\r'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'\\n'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'\\t'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'H'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'e'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b' '], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'p'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'o'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b't'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b's'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b' '], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b't'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'h'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'e'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b' '], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'C'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'L'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'i'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'e'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'f'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b't'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'o'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'o'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'n'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b' '], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'i'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b't'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b' '], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'i'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'n'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b' '], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'h'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'i'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b's'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b' '], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'h'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'a'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'n'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'd'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b's'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'.'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b' '], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b' '], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'I'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'n'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b' '], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b't'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'h'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'e'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b' '], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'c'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'h'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'a'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'l'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'l'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'-'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'd'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'r'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'e'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b's'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b't'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b's'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b','], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=string, numpy=array([b' '], dtype=object)>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d95cc35fbf3c9f0b80ca1959cb9ff74b242637f9e12377a7e797ff02e3d7ef3f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
